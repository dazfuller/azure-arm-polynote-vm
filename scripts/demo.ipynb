{
  "metadata" : {
    "config" : {
      "dependencies" : {
        
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "master" : "local[*]",
        "spark.driver.memory" : "2g"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# Demo\n",
        "\n",
        "\n",
        "This shows using Python to read data from the GitHub search\n",
        " API and writing it to the blob storage backed mount point\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1586594006684,
          "endTs" : 1586594009071
        },
        "language" : "python"
      },
      "language" : "python",
      "source" : [
        "import json\r\n",
        "import requests\r\n",
        "\r\n",
        "req = requests.get(\"https://api.github.com/search/repositories?q=tetris&sort=stars&order=desc\")\r\n",
        "\r\n",
        "if not req.ok:\r\n",
        "    raise \"Unable to download data\"\r\n",
        "\r\n",
        "with open(\"/media/polydata/data/github-tetris.json\", \"w\") as f:\r\n",
        "    json.dump(req.json(), f)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 5,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "Next we take the downloaded data back from the file system, read it into a Spark DataFrame, and perform the following:* Explode out the repo data\n",
        "* Filter to only the few columns we're interested in\n",
        "* Cast the numeric and timestamp values into their correct data type\n",
        "\n",
        "*Note:* Polynote has already created the SparkSession as `spark`, and has imported the functions and implicits.\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1586594014352,
          "endTs" : 1586594014982
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.sql.types.{TimestampType, IntegerType}\r\n",
        "\r\n",
        "val df = spark.read.json(\"/media/polydata/data/github-tetris.json\")\r\n",
        "    .withColumn(\"items\", explode($\"items\"))\r\n",
        "    .select(\"items.*\")\r\n",
        "    .select(\"full_name\", \"description\", \"fork\", \"language\", \"open_issues\", \"updated_at\", \"watchers\")\r\n",
        "    .withColumn(\"open_issues\", $\"open_issues\".cast(IntegerType))\r\n",
        "    .withColumn(\"watchers\", $\"watchers\".cast(IntegerType))\r\n",
        "    .withColumn(\"updated_at\", $\"updated_at\".cast(TimestampType))"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 6,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "Now we perform a simple aggregation to get a view of the number of projects by language type, and the number of projects, issues, and watchers for each language."
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1586594127823,
          "endTs" : 1586594130236
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "df.groupBy($\"language\")\r\n",
        "    .agg(\r\n",
        "        count($\"full_name\").alias(\"num_projects\"), \r\n",
        "        sum($\"open_issues\").alias(\"total_open_issues\"), \r\n",
        "        sum(\"watchers\").alias(\"total_watchers\"))\r\n",
        "    .orderBy($\"total_watchers\".desc)\r\n",
        "    .show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+----------+------------+-----------------+--------------+\n",
            "|  language|num_projects|total_open_issues|total_watchers|\n",
            "+----------+------------+-----------------+--------------+\n",
            "|JavaScript|          10|               56|         13272|\n",
            "|     Swift|           2|                5|          1633|\n",
            "|    Python|           4|                3|          1286|\n",
            "|       C++|           3|               10|          1271|\n",
            "|  Assembly|           2|                2|           890|\n",
            "|      Dart|           1|                0|           839|\n",
            "|   Clojure|           2|               17|           581|\n",
            "|   Haskell|           1|                0|           422|\n",
            "|       Lua|           1|                2|           413|\n",
            "|      HTML|           1|               17|           388|\n",
            "|     Shell|           1|                2|           344|\n",
            "|      Java|           1|                0|           326|\n",
            "|        Go|           1|                0|           217|\n",
            "+----------+------------+-----------------+--------------+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    }
  ]
}